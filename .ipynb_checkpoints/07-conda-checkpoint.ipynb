{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Modeling Problem and the Engineering Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a practicing data scientist, it is likely that you spend the bulk of your time working toward the development of a model for a particular inference or prediciton application. It is less likely that you spend time thinking of the equally complex problems stemming from your system infrastructure. We might trivially think of these two often orthogonal concerns as the **modeling problem** and the **engineering problem**. The typical data scientist is trained to solve the former, often in an extremely rigorous manner, but can often wind up developing a series of ad hoc solutions to the latter.\n",
    "\n",
    "Since its introduction in 2013, Docker has quickly become a fundamental tool in the design and deployment of robust engineering infrastructure for many applications. From the smallest tech shops to Google, Docker is being used to \n",
    "\n",
    "- modernize traditional software\n",
    "- leverage cloud resources for application architecture\n",
    "- streamline continuous integration and delopyment pipelines\n",
    "- build out microservices\n",
    "\n",
    "These will certainly seem downright esoteric to the Data Scientist who is typically concerned with feature importances or how many epochs to run to train a certain neural network. \n",
    "\n",
    "That said, developing a robust engineering practice with Docker at its core can only make for better data science. This includes immediate concerns such as environment configuration and replicability and presentation of results, but in learning how to use Docker properly the data scientist can ensure that their work is deployed correctly as part and product of their team's software.\n",
    "\n",
    "Here, I discuss Docker as a tool for the data scientist, in particular in conjunction with the popular interactive programming platform Jupyter and the cloud computing platform Amazon Web Services (AWS). Using Docker, Jupyter and AWS, the data scientist can take control of their environment configuration, prototype scalable data architectures, and trivially clone their work toward replicability and communication.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first chapter, you will configure your local system and create an AWS instance in order to do data science work. To do this work, you will use the package management system `conda`, the containerization technology Docker, the version control software `git` and its counterpart cloud-based backup service Github.com, and the system design tool Docker Compose. \n",
    "\n",
    "There is something of a bootstrap moment in this first chapter. I'm writing this book using Jupyter notebooks. The goal is that you are able to read it and execute the very same code from Jupyter notebooks while you're reading it. That said, you may not even have a Jupyter running on your system. The steps here I designed to take you through step-by-step to install and configure all of the tools you will need to do the work in this book. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume a basic knowledge of working in bash. This should include things like knowing that `~` is an alias for your home directory, that `pwd` shows your current location, `cd` changes directories and `ls` can be used to list files and in a directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If you are using a Mac OS X system or a Linux system, you will already have Bash available to you in an application called Terminal. If you are using a Windows system, you can use the Anaconda Prompt that will be installed next. If you are on a Mac, you may want to install iTerm (https://iterm2.com). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is recommended that you install Conda on your local system. I will be using Jupyter notebooks to drive  both my Python and Bash sessions. With a working local conda installation, you will be able to do the same. Detailed instructions for installing Conda on your system can be found here: https://conda.io/docs/user-guide/install/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple but extensible text editor such as Atom (https://atom.io)can be an invaluable tool. Atom is available for any modern operating system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `vim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly all of the work we will be doing will be on remote systems. It can be useful to be able to edit text files in place on these remote systems. We have essentially three options to do this:\n",
    "\n",
    "- Vim\n",
    "- nano\n",
    "- Emacs\n",
    "\n",
    "I will empahsize Vim. Vim is a text editor, like Atom. It is extensible, if not exactly simple. It is available on nearly every system by default, but it can be very challenging to learn. With a little guidance, we will be able to do but we need to do. It should be noted that we will only be using Vim on remote systems. If you would like to spend some time acquainting yourself with Vim, you can type `vimtutor` at a Bash prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your Local System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first chapter, with the exception of some AWS system configuration in the browser, all of the work that you will be doing will take place at the command line. Work at the command line is done via a special type of application called a shell. A shell is a user interface that provides access to the operating system of a computer. We prefer the shell to other modes of working with computers because of its simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring Your Conda Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conda is an open-source package and environment management system. We will use it here to configure and manage our Python environment on our local system, but it is also used to do the same within the Docker containers we will be using throughout. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we display the currently available `conda` environments using the command `conda env list`. A `conda` environment is a named configuration of Python packages. I have made the switch completely to be working in Python 3, but an excellent use case in the past for having different named environments was for maintaining both a Python 2 and a Python 3 configuration on the same computer. On my system I currently only have a single environment, the `base` environment.\n",
    "\n",
    "The `base` environment is installed by default when `conda` itself is installed. We will be installing a second environment to use when running Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                  *  /Users/joshuacook/miniconda3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\square$ **Note:** Occasionally in code listings, I will truncate the output. If you see `...` in the listing, this should be taken to mean that there is additional output generated that is not important for the discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I use `conda` to create a new environment. I have named this environment `jupyter` because that will be its primary use case. This nameTake a to a lot here. Back, however, has no semantic meaning and can be anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata .............\n",
      "Solving package specifications: .\n",
      "\n",
      "Package plan for installation in environment /Users/joshuacook/miniconda3/envs/jupyter:\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    ca-certificates: 2018.4.16-0      conda-forge\n",
      "    certifi:         2018.4.16-py36_0 conda-forge\n",
      "    ncurses:         5.9-10           conda-forge\n",
      "    \n",
      "    ...\n",
      "    \n",
      "#\n",
      "# To activate this environment, use:\n",
      "# > source activate jupyter\n",
      "#\n",
      "# To deactivate an active environment, use:\n",
      "# > source deactivate\n",
      "#\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda create --name jupyter python=3.6 --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having created the environment, I activate it using the `source` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(jupyter) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "source activate jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run `conda env list` again. The updated output shows that not only do we now have two environments, but the star next to the `jupyter` environment shows that it is the environment that is currently active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /Users/joshuacook/miniconda3\n",
      "jupyter               *  /Users/joshuacook/miniconda3/envs/jupyter\n",
      "\n",
      "(jupyter) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "conda env list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use `conda` to install `jupyter` and the packages upon which it depends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false,
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching package metadata ...\n",
      "Solving package specifications: .\n",
      "Package plan for installation in environment /Users/joshuacook/miniconda3/envs/jupyter:\n",
      "The following NEW packages will be INSTALLED:\n",
      "alabaster:\n",
      "asn1crypto:\n",
      "babel:\n",
      "...\n",
      "0.7.10-py36h306e16b_0 anaconda\n",
      "0.24.0-py36_0         anaconda\n",
      "2.5.3-py36_0          anaconda\n",
      "ipywidgets-7.2 100% |################################| Time: 0:00:00  1.67 MB/s\n",
      "jupyter-1.0.0- 100% |################################| Time: 0:00:00  3.59 MB/s\n",
      "(jupyter)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "conda install -c anaconda jupyter --yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (ADVANCED) Install the Bash Kernel\n",
    "\n",
    "Optionally, we install the bash kernel, so that we can run bash commands from a Jupyter Notebook. Note, that this is what I have done with the notebooks I am using to write this text. You can follow along into notebooks and run the commands here as bash commands but did you sow you must install the bash kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false,
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bash_kernel\n",
      "  Using cached https://files.pythonhosted.org/packages/93/7a/50edf4a05663429b4ca6e789a10fd3d1b581ec869a036b9d7d9ba1ffc34a/bash_kernel-0.7.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pexpect>=4.0 in /Users/joshuacook/miniconda3/envs/jupyter/lib/python3.6/site-packages (from bash_kernel)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/joshuacook/miniconda3/envs/jupyter/lib/python3.6/site-packages (from pexpect>=4.0->bash_kernel)\n",
      "Installing collected packages: bash-kernel\n",
      "Successfully installed bash-kernel-0.7.1\n",
      "\u001b[33mYou are using pip version 9.0.3, however version 10.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "(jupyter) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "pip install bash_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false,
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing IPython kernel spec\n",
      "(jupyter) "
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "python -m bash_kernel.install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (ADVANCED) Launch Jupyter from the Home Directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you wish to follow along using these Jupyter notebooks, you should launch Jupyter from the home directory on your system. On every system that we will use, the `~` symbol is used as an alias for your home directory. The location of the actual home directory will vary by system. Ubuntu users' home directory will be \\texttt{home/username}. Mac OS X users' home directory will be \\texttt{/Users/username}. Windows/Git-Bash users' home directory will be \\texttt{/c/Users/username}.}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SSH Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the work that you will be doing will take place remotely. As such, there is very little configuration to be done for the local system. The one thing that you will need to do is configure a set of SSH Keys to enable secure connection to the remote system you bring online. We will be using the Secure Shell protocol (SSH) to do the vast majority of our command line work. It is considered a best practice to use an SSH key pair for authentication when using SSH. An ssh key pair is a set of two long character strings: a private key and a public key. Though they are both called keys, I prefer to think of them as a key and lock. \n",
    "\n",
    "![Connecting with SSH Keys](doc/img/ch-01-ssh_keys.png){#fig:ssh_keys}\n",
    "\n",
    "An SSH Key is a password-less method of authenticating to a remote system using public-key cryptography. Authentication is done using a key pair consisting of a public key (`id_rsa.pub`), which can be shared publicly, and a private key (`id_rsa`), which is known only to the user (See [@fig:ssh_keys]). One might think of the public key as the lock on your front door, accessible to anyone, and the private key as the key in your pocket so that only you are able to open your door and gain access to your home.\n",
    "\n",
    "You will generate this key pair on our local system and then provide the public key to AWS so that it can be added to any system you wish to launch. You will keep the private key on our local system and use it whenever you wish to gain access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a New Key Pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will use the Bash tool `ssh-keygen` to create a new key pair. To begin open a new terminal session\\footnote{On a Mac or Linux system, simply open the Terminal application. On Windows, open Git-Bash. On Chromebook, open Termius.}, where you will examine whether or not you already have a key pair. Launching a new Bash session will put us in our home directory. The canonical location for storing SSH Keys is in a folder called `~/.ssh` in our home directory. Note that this directory begins with a `.` which makes it a hidden directory. In [@lst:ls_home], you use `cd` to navigate to our home directory and `ls -la` to display all of the contents of our home directory in a list.\n",
    "\n",
    "We will first check to make sure that you do not already have an SSH key pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### List the contents of our home directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "drwxr-xr-x   21 joshuacook  staff    714 Jul 31  2017 .pylint.d\n",
      "drwx------   11 joshuacook  staff    374 Jan 28 18:49 .ssh\n",
      "drwxr-xr-x    6 joshuacook  staff    204 Feb  2 22:01 .vim\n",
      "-rw-------    1 joshuacook  staff  20788 Feb 10 08:54 .viminfo\n",
      "-rw-r--r--@   1 joshuacook  staff   1263 Jul 26  2017 .vimrc\n",
      "drwx------@   5 joshuacook  staff    170 Aug 26 09:12 Applications\n",
      "drwx------+  19 joshuacook  staff    646 Feb 11 09:32 Desktop\n",
      "drwx------+   6 joshuacook  staff    204 Feb  4 12:18 Documents\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "ls -la ~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display contents of the `.ssh` directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My local system is running Mac OS X and has the `.ssh` folder already. As the directory already exists, in [@lst:ls_ssh], I list the contents of my `.ssh` directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 24\n",
      "drwxr-xr-x  8 jovyan users  272 Apr 24 23:07 .\n",
      "drwxr-xr-x 21 jovyan users  714 Apr 25 00:39 ..\n",
      "-rw-------  1 jovyan users 1679 Apr 24 23:06 id_rsa\n",
      "-rw-r--r--  1 jovyan users  418 Apr 24 23:06 id_rsa.pub\n"
     ]
    }
   ],
   "source": [
    "ls -la ~/.ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, I already have an SSH Keypair named `id_rsa` and `id_rsa.pub`. If this is true for you, as well, you should skip the next step and not create new SSH Keys (See [@lst:create_new_ssh_key]).\n",
    "\n",
    "If when listing the home directory, you do not see a folder called `.ssh` or when displaying the contents of `.ssh` you do not see an SSH Keypair named `id_rsa` and `id_rsa.pub`, a new SSH Keypair will need to be created. In [@lst:create_new_ssh_key], you create a new SSH Keypair using the `ssh-keygen` command line utility.\n",
    "\n",
    "During the creation of the SSH Keypair, you will be prompted three times. The first asks where you should save the SSH Keypair, defaulting to the `.ssh/id_rsa` in our home directory. In [@lst:create_new_ssh_key], you see that this is being done at `/Users/joshuacook/.ssh/id_rsa` on my local system where my username is `joshuacook`. The second and third prompts will ask for a passphrase to be added to the key. For our purposes, leaving this passphrase empty will be fine. In other words, the default options are preferable and you may simply hit `<ENTER>` three times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a new SSH Keypair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating public/private rsa key pair.\n",
      "Enter file in which to save the key (/Users/joshuacook/.ssh/id_rsa): \n",
      "Enter passphrase (empty for no passphrase):\n",
      "Enter same passphrase again:\n",
      "Your identification has been saved in id_rsa.\n",
      "Your public key has been saved in id_rsa.pub.\n",
      "The key fingerprint is:\n",
      "SHA256:p5KeEomPt6izFC5gaFphfx3zw8aAB+D8RiUA1/nEsUc joshuacook@LOCAL\n",
      "The key's randomart image is:\n",
      "+---[RSA 2048]----+\n",
      "|  ..++ooo.E      |\n",
      "|   +  o=oo       |\n",
      "|  o o oo* .      |\n",
      "|.. o o o.O       |\n",
      "|o+....+ S B      |\n",
      "|*.o oo . + .     |\n",
      "|oo o .o .        |\n",
      "|+ ..+. o         |\n",
      "|o+...oo          |\n",
      "+----[SHA256]-----+\n"
     ]
    }
   ],
   "source": [
    "ssh-keygen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can verify the SSH Keypair you just created by displaying the Public Key in our shell ([@lst:cat_pub_key]). Here, you use the `cat` command, which concatenates the contents of `id_rsa.pub` to the shell output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Display Public SSH Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssh-rsa\n",
      "AAAAB3NzaC1yc2EAAAADAQABAAABAQDdnHPEiq1a4OsDDY+g9luWQS8pCjBmR\n",
      "64MmsrQ9MaIaE5shIcFB1Kg3pGwJpypiZjoSh9pS55S9LckNsBfn8Ff42ALLj\n",
      "R8y+WlJKVk/0DvDXgGVcCc0t/uTvxVx0bRruYxLW167J89UnxnJuRZDLeY9fD\n",
      "OfIzSR5eglhCWVqiOzB+OsLqR1W04Xz1oStID78UiY5msW+EFg25Hg1wepYMC\n",
      "JG/Zr43ByOYPGseUrbCqFBS1KlQnzfWRfEKHZbtEe6HbWwz1UDL2NrdFXxZAI\n",
      "XYYoCVtl4WXd/WjDwSjbMmtf3BqenVKZcP2DQ9/W+geIGGjvOTfUdsCHennYI\n",
      "EUfEEP joshuacook@LOCAL\n"
     ]
    }
   ],
   "source": [
    "cat ~/.ssh/id_rsa.pub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the sum of the local configuration you will need to do in order to get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Web Services\n",
    "\n",
    "If you have not already done so, set up an AWS account\\footnote{As of 2017/12/19, detailed instructions for doing this can be obtained here: https://aws.amazon.com/premiumsupport/knowledge-center/create-and-activate-aws-account/}. You will be using AWS to manage the hardware upon which your data science platform will run. We will leave the details of what exactly \"hardware\" means to AWS. This is to say that AWS may be allocating resources as a virtual machine, but for your purposes, the experience will be as if you are using a physical system across the room from you.\n",
    "\n",
    "The most popular service offered by Amazon Web Services is the Elastic Compute Cloud (EC2), \"a web service that provides secure, resizable compute capacity in the cloud\"\\footnote{https://aws.amazon.com/ec2/}. For our purposes, compute capacity means a cloud-based computer you will use to run your platform.\n",
    "\n",
    "If you are new to AWS you will be able to work through this text using the AWS Free Tier\\footnote{https://aws.amazon.com/free/}. For the first 12 months following sign up, new users receive 750 Hours per month of EC2 time. This amounts to 31.25 days of availability and, provided that readers keep only one server running at a time, ensures that readers can work through this text at no cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure your AWS Account\n",
    "\n",
    "That is the sum of the local configuration you will need to do in order to get started. The next thing you will need to do is configure our AWS Account. To do this, you will need to configure a Key Pair corresponding to the SSH Keypair on your local system. The AWS Key Pair is slightly misnamed as it is not in fact a pair, but rather is simply the public portion of the SSH Keypair you have on our local system.\n",
    "\n",
    "To begin, log in to your AWS control panel and navigate to the EC2 Dashboard ([@fig:access_ec2_dash]). First, access “Services” ([@fig:access_ec2_dash], #1) then access “EC2” ([@fig:access_ec2_dash], #2). The Services link can be accessed from any page in the AWS website.\n",
    "\n",
    "![Access EC2 Dashboard](doc/img/ch-01-access_ec2_dash.png){#fig:access_ec2_dash}\n",
    "\n",
    "### Add the Public Key to AWS\n",
    "\n",
    "Once at the EC2 control panel, access the Key Pairs pane using either link ([@fig:access_key_pairs]).\n",
    "\n",
    "![Access Key Pairs in the EC2 Dashboard](doc/img/ch-01-access_key_pairs.png){#fig:access_key_pairs}\n",
    "\n",
    "From the Key Pairs pane, choose “Import Key Pair.” This will activate a modal that you can use to create a new key pair associated with a region on your AWS account. Make sure to give the key pair a computer-friendly name, like `from-MacBook-2018`. Paste the contents of your public key (`id_rsa.pub`) into the public key contents. Prior to clicking Import, your key should appear as in [@fig:import_public_key]. Click Import to create the new key.\n",
    "\n",
    "![Import a New Public Key](doc/img/ch-01-import_public_key.png){#fig:import_public_key}\n",
    "\n",
    "You have created a key pair between AWS and your local system. When you create a new instance, you will instruct AWS to provision the instance with this public key and thus you will be able to access the cloud-based system from your local system using your private key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch a New EC2 Instance\n",
    "\n",
    "To create a new instance, start from the EC2 Dashboard and click the Launch Instance button ([@fig:launch_instance]).\n",
    "\n",
    "![Begin the launch process for a new instance](doc/img/ch-01-launch_instance.png){#fig:launch_instance}\n",
    "\n",
    "### Step 1: Choose an Amazon Machine Image (AMI)\n",
    "\n",
    "The launching of a new instance is a multi-step process that walks the user through all configurations necessary. The **first tab** is “Choose AMI.” An AMI is an Amazon Machine Image\\footnote{http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html}. and contains the software you will need to run your sandbox machine. I recommend choosing the latest stable Ubuntu Server release that is free-tier eligible. At the time of writing, this was ami-efd0428f, Ubuntu Server 16.04 LTS (HVM), SSD Volume Type ([@fig:latest_ubuntu]).\n",
    "\n",
    "![Choose the latest stable Ubuntu Server release as AMI](doc/img/ch-01-latest_ubuntu.png){#fig:latest_ubuntu}\n",
    "\n",
    "### Step 2: Choose Instance Type\n",
    "The **second tab** is “Choose Instance Type.” In practice, I have found that the free tier, `t2.micro`  ([@fig:choose_type]), is sufficient for many applications. Furthermore, the instance type may always be changed later should the need present itself.\n",
    "\n",
    "![Choose `t2.micro` for Instance Type](doc/img/ch-01-choose_type.png){#fig:choose_type}\n",
    "\n",
    "### Step 3: Configure Instance Details\n",
    "\n",
    "The **third tab**, “Configure Instance,” can be safely ignored.\n",
    "\n",
    "### Step 4: Add Storage\n",
    "\n",
    "The **fourth tab** is “Add Storage.” This option is also specific to intended usage. It should be noted that Jupyter Docker images can take up more than 5GB of disk\n",
    "space in the local image cache. For this reason, it is recommended to raise the value from the default 8GB to 30GB. Furthermore, as noted on this tab:\n",
    "\n",
    "> Free tier eligible customers can get up to 30 GB of EBS General Purpose (SSD) or Magnetic storage.\n",
    "\n",
    "### Step 5: Add Tags\n",
    "\n",
    "The fifth tab, “Add Tags,” can be safely ignored.\n",
    "\n",
    "### Step 6: Configure Security Group\n",
    "\n",
    "The sixth tab, “Configure Security Group,” is critical for the proper functioning of your systems. By default this tab will be set up to \"Create a **new** security group\". This will not work for us! Ultimately, we will be accessing our system via a web browser which we require at a minimum that port 80 is open. We recommend simply using the default group which will open our system on all ports. If greater security is required for your specific application a more restrictive security group may be defined and used.\n",
    "\n",
    "Select the \"default\" security group ([@fig:default_security_group]).\n",
    "\n",
    "![Choose the latest stable Ubuntu Server release as AMI](doc/img/ch-01-default_security_group.png){#fig:default_security_group}\n",
    "\n",
    "$\\square$ **Note:** You may receive a Warning stating, \"Rules with source of 0.0.0.0/0 allow all IP addresses to access your instance. We recommend setting security group rules to allow access from known IP addresses only.\" This is expected and is okay.\n",
    "\n",
    "### Step 7: Review Instance Launch\n",
    "\n",
    "Finally, click “Review and Launch.” Here, you see the specific configuration of the EC2 instance you will be creating. Verify that you are creating a `t2.micro` ([@fig:review_and_launch], #2)running the latest free tier-eligible version of Ubuntu Server ([@fig:review_and_launch], #1)and that it is available to all traffic ([@fig:review_and_launch], #3), and then click the Launch button ([@fig:review_and_launch], #4).\n",
    "\n",
    "![Review and launch the new instance](doc/img/ch-01-review_and_launch.png){#fig:review_and_launch}\n",
    "\n",
    "### Add an SSH Key\n",
    "\n",
    "In a final confirmation step, you will see a modal titled “Select an existing key pair or create a new key pair.” Select the key pair you previously created. Check the box acknowledging access to that key pair and launch the instance ([@fig:add_key_pair]).\n",
    "\n",
    "![Add a key pair to the instance](doc/img/ch-01-add_key_pair.png){#fig:add_key_pair}\n",
    "\n",
    "$\\square$ **Note:** If this step is not done correctly, that is, if the correct key pair is not added to the launching instance, the instance will need to be terminated and a new instance will need to be launched. There is now way to add a key pair to a running instance.\n",
    "\n",
    "You should see a notification that the instance is now running. Click the View Instances tab in the lower right corner to be taken to the EC2 Dashboard Instances pane, where you should see your new instance running.\n",
    "\n",
    "### Examing the Newly Launched Instance\n",
    "\n",
    "Make note of the IP address of the new instance ([@fig:new_ip]).\n",
    "\n",
    "![Add a key pair to the instance](doc/img/ch-01-new_ip.png){#fig:new_ip}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with our new ssh key, we will add the public key to Github so that we can use the key to access our github account. \n",
    "\n",
    "1. copy the public key \n",
    "1. choose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we will do is create a new local `git` repository and a remote Github repository. The local repository is where we will do all of our work. We will use the remote Github repository to track all of the work that we will do. It is a common beginning misconception to think of `git` and Github as the same thing. `git` is a commond line tool we will use to track changes to our project. Github is a cloud-based service designed to work with `git` to help us to make sure our work is always backed up on an additional system. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the new repository on Github\n",
    "\n",
    "At github.com, in the upper-right hand corner, click the plus icon and select \"New Repository\". \n",
    "\n",
    "![](doc/img/newrepo.png)\n",
    "\n",
    "Give your new repository a name. It does not need to match `engineering-for-data-science`, but it is generally considered a best practice to make sure the repository on Github and the local directory containing your files have the same name. You do not need to provide a description nor create a `README` file.\n",
    "\n",
    "Finally, click \"Create Repository\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](doc/img/Create_a_New_Repository.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a new directory\n",
    "\n",
    "We use the `mkdir` command to create a new directory to hold our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [],
   "source": [
    "mkdir -p engineering-for-data-science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change directories to the new directory\n",
    "\n",
    "Next, use the `cd` (change directory) command to change directories to the new directory we just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [],
   "source": [
    "cd engineering-for-data-science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized empty Git repository in /home/jovyan/engineering-for-data-science/.git/\n"
     ]
    }
   ],
   "source": [
    "git init "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [],
   "source": [
    "mkdir chapter-01-introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [],
   "source": [
    "echo \".ssh\" > .gitignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chapter-01-introduction\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4\n",
      "drwxr-xr-x  5 jovyan users 170 Apr 24 23:53 .\n",
      "drwxr-xr-x 20 jovyan users 680 Apr 24 23:51 ..\n",
      "drwxr-xr-x  2 jovyan users  68 Apr 24 23:52 chapter-01-introduction\n",
      "drwxr-xr-x 10 jovyan users 340 Apr 24 23:51 .git\n",
      "-rw-r--r--  1 jovyan users   5 Apr 24 23:53 .gitignore\n"
     ]
    }
   ],
   "source": [
    "ls -la"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Please tell me who you are.\n",
      "\n",
      "Run\n",
      "\n",
      "  git config --global user.email \"you@example.com\"\n",
      "  git config --global user.name \"Your Name\"\n",
      "\n",
      "to set your account's default identity.\n",
      "Omit --global to set the identity only in this repository.\n",
      "\n",
      "fatal: empty ident name (for <jovyan@eb5eb401ee58.(none)>) not allowed\n"
     ]
    }
   ],
   "source": [
    "git add .gitignore && git commit -m 'initialize project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [],
   "source": [
    "git config --global user.email \"me@joshuacook.me\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [],
   "source": [
    "git config --global user.name \"Joshua Cook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[master (root-commit) 970c61e] initialize project\n",
      " 1 file changed, 1 insertion(+)\n",
      " create mode 100644 .gitignore\n"
     ]
    }
   ],
   "source": [
    "git add .gitignore && git commit -m 'initialize project'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [],
   "source": [
    "git remote add origin git@github.com:joshuacook/engineering-for-data-science.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "bash",
     "local"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting objects: 3, done.\n",
      "Writing objects: 100% (3/3), 222 bytes | 0 bytes/s, done.\n",
      "Total 3 (delta 0), reused 0 (delta 0)\n",
      "To git@github.com:joshuacook/engineering-for-data-science.git\n",
      " * [new branch]      master -> master\n",
      "Branch master set up to track remote branch master from origin.\n"
     ]
    }
   ],
   "source": [
    "git push -u origin master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](doc/img/joshuacook_engineering-for-data-science.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git and Github\n",
    "\n",
    "As you work through this text, you will be developing a series of data science projects. Tracking software development work is typically done using version control software. One of the most popular version control tools is `git`. Additonally, it can be useful to use a version control hosting service as a remote backup for work being tracked using `git`. The remote service we will use is Github.com. In my experience, learners who are new to version control often confuse `git` and Github, so it bares repeating -- we will use `git` to track changes we make to our code and Github as a remote backup for these changes.\n",
    "\n",
    "## Configuring Github\n",
    "\n",
    "We will assume that you have a Github account. Once this has been done, you will need to configue an SSH connection between AWS and Github. This next part may potentially create a confusion. We are actually going to need a new SSH Keypair, this one associated with our AWS instance. This is because it is our AWS instance that will be connecting to Github, not our local machine (See [@fig:ssh_local_remote]).\n",
    "\n",
    "![SSH Connections](doc/img/ch-01-ssh_local_remote.png){#fig:ssh_local_remote}\n",
    "\n",
    "### Create a New Key Pair\n",
    "\n",
    "In [@lst:create_new_ssh_key_remote], you create a new key pair on your remote AWS instance. In [@lst:ssh_into_new_instance], you connect to your new AWS instance. To do this we will use the IP address we made note of in [@fig:new_ip]. We use SSH to connect to our remote AWS instance. Note that we use the username, `ubuntu`, the default username for the Ubuntu 16 AMI provided by AWS.\n",
    "\n",
    "Listing: Create a new SSH Keypair\n",
    "\n",
    "```{#lst:ssh_into_new_instance}\n",
    "$ ssh ubuntu@54.244.109.176\n",
    "Welcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-64-generic x86_64)\n",
    "```\n",
    "\n",
    "$\\square$ **Note:** The first time you access your EC2 instance, you should see the following message: `The authenticity of host '54.244.109.176 (54.244.109.176)' can't be established ... Are you sure you want to continue connecting (yes/no)? This is expected. You should hit <ENTER> to accept or type yes and hit <ENTER>.`\n",
    "\n",
    "In [@lst:create_new_ssh_key_remote], you create a new SSH Keypair on our remote AWS instance. Again, during the creation of the SSH Keypair, you will be prompted three times. The first asks where you should save the SSH Keypair, defaulting to the `.ssh/id_rsa` in our home directory. In [@lst:create_new_ssh_key_remote], you see that this is being done at `/home/ubuntu/.ssh/id_rsa`\\footnote{This should be the same for everyone now, as you should be working on an AWS \\texttt{t2.micro} running an Ubuntu system where the user's name is \\texttt{ubuntu}}. The second and third prompts will ask for a passphrase to be added to the key. For our purposes, leaving this passphrase empty will be fine. In other words, the default options are preferable and you may simply hit `<ENTER>` three times.\n",
    "\n",
    "Listing: Create a new SSH Keypair\n",
    "\n",
    "```{#lst:create_new_ssh_key_remote}\n",
    "$ ssh-keygen\n",
    "Generating public/private rsa key pair.\n",
    "Enter file in which to save the key (/home/ubuntu/.ssh/id_rsa):\n",
    "Created directory '/home/ubuntu/.ssh'.\n",
    "Enter passphrase (empty for no passphrase):\n",
    "Enter same passphrase again:\n",
    "Your identification has been saved in /home/ubuntu/.ssh/id_rsa.\n",
    "Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\n",
    "The key fingerprint is:\n",
    "SHA256:ZSpFpgSRgRqlQom8yVBG2dZo1tgkPQdrmUGgMXGDtRY\n",
    "ubuntu@ip-172-31-43-19\n",
    "The key's randomart image is:\n",
    "+---[RSA 2048]----+\n",
    "|o=XBE/*.o        |\n",
    "|==+=O**O.        |\n",
    "|=o++o *o. o      |\n",
    "|o+ . . . +       |\n",
    "|      . S        |\n",
    "|       .         |\n",
    "|                 |\n",
    "|                 |\n",
    "|                 |\n",
    "+----[SHA256]-----+\n",
    "```\n",
    "\n",
    "As before, you can verify the SSH Keypair you just created by displaying the Public Key in your shell ([@lst:cat_pub_key_remote]). Again, you use the `cat` command, which concatenates the contents of `id_rsa.pub` to the shell output.\n",
    "\n",
    "Listing: Display Public SSH Key\n",
    "\n",
    "```{#lst:cat_pub_key_remote}\n",
    "$ cat ~/.ssh/id_rsa.pub\n",
    "ssh-rsa\n",
    "AAAAB3NzaC1yc2EAAAADAQABAAABAQDQ896GUMgCMAIW79gwF3ojRjcUYCKUKc8b+q\n",
    "iQlah2jtr7s0K4WRGjktOy3lCCHO+1UK/GrzY1Y4VxCKoKJDH3G9N5UzyGhlxa/2Ah\n",
    "kKxzHht1knyh/mkVGqYUhuHpXfxUQAstCFrIdp3G0MDPiko2qeJcBF7JSv1lLMbIuM\n",
    "XuVU/Mzq6BU+tEogScYytmLckyEe1j8RJ+e5nBURwmkgj3UAN1DzmU/lVwLlltEpmC\n",
    "DlOel4yEXAw8yBwM3GwjahfiBThvBHpsc43HxWrkM8Yi/kdDnvsDZYxU4zhXZPsPab\n",
    "UY/LfxEod9c6Sui5W8GtAfdi6krnqbzxrKt81Mradh ubuntu@ip-172-31-43-19\n",
    "```\n",
    "\n",
    "### Add the Public Key to Github\n",
    "\n",
    "Previously, you added your local SSH public key to your AWS account. Now, you will add your AWS SSH public key to your Github account\\footnote{https://help.github.com/articles/adding-a-new-ssh-key-to-your-github-account/}. First, access the **Settings** for your account by clicking the profile photo in the upper-right corner of any page on Github. Next, in the user settings sidebar, select **SSH and GPG keys**. On the SSH and GPG Keys page, click **New SSH key**. On the next page, give your key a descriptive title e.g. \"AWS Feb 2018\" and then paste your AWS public key in to the \"Key\" field . Finally, click **Add SSH key** and confirm your Github password , if prompted.\n",
    "\n",
    "### Learning to read the Bash Prompt\n",
    "\n",
    "During your work you will no doubt notice that an idle SSH connection may become disconnected and/or unresponsive. Should this happen, simply close the terminal session, launch a new one, and reconnect to the remote instance. \n",
    "\n",
    "The most important thing is that you are aware of which system your current shell session is connected to. Shell prompts are designed to relay this information to you immediately. If you are new to working with Bash, you may need to train yourself to being aware of the prompt when typing. [@lst:default_AWS_prompt] shows the default AWS Bash prompt. The information contained is the username, `ubuntu`, and the private IP address of the AWS instance. **This is not the public address you use to connect**. What is useful about this, is that we can immediately see that the user is `ubuntu`. This tells us we are connected to AWS.\n",
    "\n",
    "Listing: The default AWS Bash prompt\n",
    "```{#lst:default_AWS_prompt}\n",
    "ubuntu@ip-172-31-21-89:~$\n",
    "```\n",
    "\n",
    "Your local system will no doubt display something different (See [@lst:other_prompt]). Again, the important thing is to take note of what is displayed by the prompt and to learn to associate that prompt with the correct system. As you become a more advanced Bash user, you may wish to personalize your prompt, but for now it is imperative that you learn to read the prompt in order to always know to which system you are connected.\n",
    "\n",
    "Listing: A local Bash prompt\n",
    "```{#lst:other_prompt}\n",
    "joshuas-macbook-pro:~$\n",
    "```\n",
    "\n",
    "### Test your SSH Connection to Github\n",
    "\n",
    "Having added you AWS Public Key to your Github account, you should verify your SSH connection from your AWS instance. In [@lst:verify_github_ssh], we attempt to connect to Github via SSH. As before, we receive a message about the authenticity of the connection. Again, type `yes`, and continue. If successful, you will see a message telling you have successfully authenticated but that Github does not provide shell access.\n",
    "\n",
    "Listing: Verify Github SSH Key\n",
    "```{#lst:verify_github_ssh}\n",
    "ubuntu@ip-172-31-21-89:~$ ssh -T git@github.com\n",
    "The authenticity of host 'github.com (IP ADDRESS)' can't be\n",
    "established.\n",
    "RSA key fingerprint is\n",
    "16:27:ac:a5:76:28:2d:36:63:1b:56:4d:eb:df:a6:48.\n",
    "Are you sure you want to continue connecting (yes/no)? yes\n",
    "Hi username! You've successfully authenticated, but GitHub does\n",
    "not provide shell access.\n",
    "```\n",
    "\n",
    "## Docker \\& Docker Compose\n",
    "\n",
    "Having configured our SSH connections and provisioned a new AWS EC2 instance, it is time to get to the business of building your data science platform. To do this you will use the containerization platform Docker and its Docker Compose tool. While Docker is very easy to use, it can be difficult to understand for the uninitiated. In an earlier work, *Docker for Data Science*\\footnote{https://www.apress.com/us/book/9781484230114}, I wrote:\n",
    "\n",
    "> [Using Docker] we add a layer of complexity to our software, but in doing so gain the advantage of ensuring that our local development environment will be identical to any possible environment into which we would deploy the application.\n",
    "\n",
    "It may be simpler, however, to simply think about using Docker as a way to manage a running process. Your system will be running two processes: an IPython shell and a PostgreSQL server. Were you to not use Docker, you would need to ensure that the AWS instance had all of the libraries required to run both of those processes (and keep those libraries up to date).\n",
    "\n",
    "Instead, you will let Docker manage the processes using a container for each process. Each respective container will be run using a predefined image built using best practices and ready to run their respective process. The exchange is this: you will take on the congitive burden of *understanding* what Docker is doing and Docker (and the Docker community) will take over the burden of making sure that your processes run.\n",
    "\n",
    "### Docker Compose\n",
    "\n",
    "Docker Compose is a tool built for managing an application consisting of multiple containers. Using Docker Compose, it is possible to completely define an application using a simple text file. To make this conversation less abstract, let's have a look at the `docker-compose.yml` file you will use to define your first application (See [@lst:docker_compose]).\n",
    "\n",
    "Listing: Your Data Science Application\n",
    "\n",
    "```{#lst:docker_compose}\n",
    "version: \"3\"\n",
    "services:\n",
    "  ipython_shell:\n",
    "    image: jupyter/scipy-notebook\n",
    "  database:\n",
    "    image: postgres\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "volumes:\n",
    "  postgres_data\n",
    "```\n",
    "\n",
    "That's it. This simple file completely defines a fully-functioning Data Science Application. In it, we define the two services we need: `ipython_shell` and `database`. These two services are defined using the `jupyter/scipy-notebook` and `postgres` images. When we launch the application, the images will be pulled from Docker Hub into our local memory and then launched. The one other thing we do is create a data volume `postgres_data`. We will use this as the data volume for our database server so that if for some reason we have to shut our system down, we do not lose our data. The data will exist on this volume independent of the services.\n",
    "\n",
    "$\\square$ **Note:** Throughout this text, when discussing infrastructure, I may casually refer to containers, services, and processes. At the risk of annoying your local site reliability engineer, you may treat these as terms as synonomous. Care should be taken, however, not to confuse services/containers/processes and images. An image defines a service, but a service should be thought of as a living and active thing. You may loosely compare the service-image relationship to the object-class relationship in Object-Oriented Programming. A service is a running container defined by an image, just like an object is an instance of a class that exists in memory.\n",
    "\n",
    "### Installing and Configuring Docker\n",
    "\n",
    "Installing Docker on your AWS instance is a downright trivial process. It consists of running an install script that can be obtained from Docker and then adding your user to the Docker group. In [@lst:install_docker], we run these two commands. First, we download the install script from https://get.docker.com, then immediately pipe the script into the shell (`| sh`).\n",
    "\n",
    "$\\square$ **Note:** It is generally considered to be a significant security vulnerability to execute arbitrary code obtained from an unknown, or untrusted source. For our purposes, the source (https://get.docker/com) is considered trustworthy, we are using SSL to perform the curl, and in practice this is the method I use to install Docker. Still, it may make the security minded more comfortable to `curl` the script, inspect, and then run it.\n",
    "\n",
    "Listing: Install Docker via a Shell Script\n",
    "\n",
    "```{#lst:install_docker}\n",
    "$ curl -sSL https://get.docker.com/ | sh\n",
    "# Executing docker install script, commit: 1d31602\n",
    "+ sudo -E sh -c apt-get update -qq >/dev/null\n",
    "...\n",
    "\n",
    "Client:\n",
    " Version:   18.02.0-ce\n",
    " API version:   1.36\n",
    " Go version:    go1.9.3\n",
    " Git commit:    fc4de44\n",
    " Built: Wed Feb  7 21:16:33 2018\n",
    " OS/Arch:   linux/amd64\n",
    " Experimental:  false\n",
    " Orchestrator:  swarm\n",
    "\n",
    "Server:\n",
    " Engine:\n",
    "  Version:  18.02.0-ce\n",
    "  API version:  1.36 (minimum version 1.12)\n",
    "  Go version:   go1.9.3\n",
    "  Git commit:   fc4de44\n",
    "  Built:    Wed Feb  7 21:15:05 2018\n",
    "  OS/Arch:  linux/amd64\n",
    "  Experimental: false\n",
    "\n",
    "...\n",
    "\n",
    "```\n",
    "\n",
    "When the script completes there is one last thing to be done. In [@lst:add_to_docker_group], you add the `ubuntu` user to the `docker` group. By default, the command line docker client will require sudo access in order to issue commands to the docker daemon. You can add the `ubuntu` user to the `docker` group in order to allow the `ubuntu` user to issue commands to docker without sudo.\n",
    "\n",
    "Listing: Add the Ubuntu User to the Docker Group\n",
    "\n",
    "```{#lst:add_to_docker_group}\n",
    "$ sudo usermod -aG docker ubuntu\n",
    "```\n",
    "\n",
    "Finally, in order to force the changes to take effect, you should disconnect and reconnect to their remote system. You can achieve this by typing `exit` or `ctrl-d` and then reconnecting via ssh to your EC2 instance.\n",
    "\n",
    "### Installing and Configuring Docker\n",
    "\n",
    "Recall that regardless of your local operating system, you are working on an AWS EC2 Instance running the Linux variant, Ubuntu. As such, `docker-compose`can be installed using the instructions provided here: https://github.com/docker/compose/releases, which are written specifically for Linux machines. As of the writing of this book, this consists of two steps.\n",
    "\n",
    "In [@lst:curl_docker_compose], you use `curl` to retrieve the `docker-compose` binary from Github. As of the writing of this book. the latest version of `docker-compose` was `1.19.0`. You should retrieve the latest version from the above url.\n",
    "\n",
    "Listing: Retrieve `docker-compose` binary from Github\n",
    "\n",
    "```{#lst:curl_docker_compose}\n",
    "$ sudo curl -L https://github.com/docker/compose/releases/download/1.19.0/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\n",
    "```\n",
    "\n",
    "In [@lst:chmod_docker-compose], we use the `chmod`\\footnote{The unix \"change mode\" utility. I pronounce it \"shmod\".} utility to allow `docker-compose` to be executed (`+x`).\n",
    "\n",
    "Listing: Enable Docker Compose to be Executed\n",
    "\n",
    "```{#lst:chmod_docker-compose}\n",
    "$ sudo chmod +x /usr/local/bin/docker-compose\n",
    "```\n",
    "\n",
    "Finally, in [@lst:docker_compose_version], we check the version of `docker-compose` against what we expect to have installed.\n",
    "\n",
    "```{#lst:docker_compose_version}\n",
    "$ docker-compose -v\n",
    "docker-compose version 1.19.0, build 9e633ef\n",
    "```\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "**TODO**\n",
    "\n",
    "## Exercises\n",
    "1. What went wrong here? What should you do?\n",
    "\n",
    "    ```\n",
    "    failed to register layer: Error processing tar file(exit status 1):\n",
    "    write / usr/bin/python2.7: no space left on device\n",
    "    ```\n",
    "\n",
    "1. What went wrong here? What should you do?\n",
    "\n",
    "    ```\n",
    "    docker: Error response from daemon: driver failed programming external\n",
    "    connectivity on endpoint cocky_swartz (08124b75d2f031def6d36c6bc819549c009\n",
    "    391e3bd76f3fe3b4e06e11be6fbad): Bind for 0.0.0.0:80 failed: port is\n",
    "    already allocated.\n",
    "    ```\n",
    "\n",
    "1. What doe this command do?\n",
    "\n",
    "    ```\n",
    "    curl -sSL https://get.docker.com | sh\n",
    "    ```\n",
    "\n",
    "1. What are two ways to display the contents of a text file from the command line?\n",
    "2. What are the two modes in vim?\n",
    "3. How do you save the changes in a file in vim?\n",
    "1. How would you find all files in your current directory that contain the string “bash”?\n",
    "1. What are the steps to launching a Jupyter Notebook Server on a running AWS instance?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
